# -*- coding: utf-8 -*-
"""LexAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G9b6SkLdrs9D5KOG7MarJYbWYuTZrHVg
"""

!pip install -q langchain chromadb sentence-transformers pymupdf pypdf langchain-community unstructured gradio openai
!pip install -q transformers torch langchain-huggingface
!pip install -q chromadb langchain-chroma

# Step 1: Install required packages
# !pip install langchain openai faiss-cpu gradio tiktoken

import os
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
import gradio as gr

# Step 2: Mount Google Drive (only for Colab)
from google.colab import drive
drive.mount('/content/drive')

# STEP 3: Load and Process PDFs
import os
from pypdf import PdfReader
from langchain_community.document_loaders import UnstructuredPDFLoader

pdf_folder_path = '/content/drive/MyDrive/Law_docs'

def load_pdfs_from_folder(folder_path):
    documents = []
    for file in os.listdir(folder_path):
        if file.endswith('.pdf'):
            file_path = os.path.join(folder_path, file)
            try:
                reader = PdfReader(file_path)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                if len(text) > 100:
                    documents.append({
                        'source': file,
                        'text': text,
                        'pages': len(reader.pages)
                    })
                else:
                    loader = UnstructuredPDFLoader(file_path)
                    data = loader.load()
                    documents.append({
                        'source': file,
                        'text': data[0].page_content,
                        'pages': 'unknown'
                    })
            except Exception as e:
                print(f"Error processing {file}: {str(e)}")
    return documents

legal_docs = load_pdfs_from_folder(pdf_folder_path)
print(f"Loaded {len(legal_docs)} PDF documents")

# STEP 4: Text Splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    separators=["\n\n", "\n", "(?<=\\. )", " ", ""]
)

document_chunks = []
for doc in legal_docs:
    chunks = text_splitter.split_text(doc['text'])
    for i, chunk in enumerate(chunks):
        document_chunks.append({
            'text': chunk,
            'source': doc['source'],
            'chunk_num': i+1,
            'metadata': {
                'pages': doc['pages'],
                'file_name': doc['source']
            }
        })

print(f"Created {len(document_chunks)} text chunks from {len(legal_docs)} documents")

# STEP 5: Embed Chunks
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

documents = [
    Document(
        page_content=chunk["text"],
        metadata={"source": chunk["source"], "chunk_num": chunk["chunk_num"]}
    ) for chunk in document_chunks
]

# Step 7: Create Chroma vector DB
from langchain_chroma import Chroma

vector_db = Chroma.from_documents(
    documents=documents,
    embedding=embedding_model,
    persist_directory="./legal_docs_chromadb"
)

!zip -r /content/legal_docs_chromadb.zip /content/legal_docs_chromadb

retriever = vector_db.as_retriever(search_kwargs={"k": 3})

!pip install -q langchain-groq
# Use GROQ + LLaMA3
from langchain_groq import ChatGroq
import os

os.environ["GROQ_API_KEY"] = "XYZ"  # â¬…ï¸ Replace with actual key

llm = ChatGroq(
    model_name="llama3-70b-8192",
    temperature=0.3
)

# Retrieval QA chain
from langchain.chains import RetrievalQA

legal_qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Gradio Interface
import gradio as gr
import re

def clean_text(text):
    text = re.sub(r"Page\\s\\d+\\s*of\\s*\\d+", "", text)
    text = re.sub(r"\\bArticle\\s\\d+[A-Z]?\\b", "", text)
    text = re.sub(r"\\d+\\.\\s*", "", text)
    text = re.sub(r"\\[\\d+\\]", "", text)
    text = re.sub(r"\\s{2,}", " ", text)
    return text.strip()

def format_prompt(user_input):
    return f"""
You are a helpful legal assistant.
Given the legal documents retrieved, answer the following question using plain, non-legal English.
Rules:
- DO NOT include any article numbers or amendment references.
- Make sure each sentence is meaningful and helpful.
Question: {user_input}
"""

def qa_chat_interface(user_input, history=[]):
    formatted_query = format_prompt(user_input)
    result = legal_qa.invoke({"query": formatted_query})
    cleaned_docs = [clean_text(doc.page_content) for doc in result["source_documents"]]
    history.append(("ðŸ‘¤ " + user_input, "âš–ï¸ " + result["result"]))
    return history, history

with gr.Blocks() as demo:
    gr.Markdown("""# ðŸ“˜ LexAI - Legal Assistant""")
    chatbot = gr.Chatbot()
    with gr.Row():
        txt = gr.Textbox(show_label=False, placeholder="Ask a legal question...")
        btn = gr.Button("Ask")

    txt.submit(qa_chat_interface, [txt, chatbot], [chatbot, chatbot])
    btn.click(qa_chat_interface, [txt, chatbot], [chatbot, chatbot])

demo.launch(share=True)
